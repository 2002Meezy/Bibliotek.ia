# ConfiguraÃ§Ã£o do LM Studio com LangChain - Bibliotek.IA

## ğŸ“‹ PrÃ©-requisitos

1. **LM Studio** instalado e rodando
2. Modelo de visÃ£o **Qwen VL** (qwen/qwen3-vl-4b) baixado
3. Servidor local do LM Studio ativado na porta **1234**

## ğŸ”§ ConfiguraÃ§Ã£o do LM Studio

### 1. Baixar o Modelo
- Abra o LM Studio
- VÃ¡ em "Discover" ou "Search"
- Procure por: `qwen3-vl-4b` ou `qwen/qwen3-vl-4b`
- Baixe o modelo (pode levar alguns minutos dependendo da conexÃ£o)

### 2. Carregar o Modelo
- Na aba "Models", selecione o modelo `qwen/qwen3-vl-4b`
- Clique em "Load Model"
- **IMPORTANTE**: Verifique se o adaptador de visÃ£o (`mmproj`) estÃ¡ ativado
  - Deve aparecer uma indicaÃ§Ã£o visual de que a visÃ£o estÃ¡ habilitada

### 3. Iniciar o Servidor Local
- VÃ¡ na aba "Local Server"
- Clique em "Start Server"
- Confirme que estÃ¡ rodando na porta **1234**
- O status deve ficar **verde** indicando que estÃ¡ online

## ğŸš€ Arquitetura LangChain

A aplicaÃ§Ã£o agora usa **LangChain** para se comunicar com o LM Studio:

### Componentes Principais

1. **ChatOpenAI**: Cliente LangChain configurado para o endpoint local
   - Base URL: `/v1` (via proxy do Vite)
   - Modelo: `qwen/qwen3-vl-4b`
   
2. **HumanMessage**: Estrutura de mensagens com suporte a visÃ£o
   - Texto: Prompt de curadoria literÃ¡ria
   - Imagem: Base64 da foto processada
   
3. **Proxy Vite**: Redirecionamento de `/v1` para `localhost:1234`
   - Resolve problemas de CORS
   - Transparente para o navegador

### Fluxo de ExecuÃ§Ã£o

```
[Browser] â†’ [Vite Dev Server:3000] â†’ [Proxy /v1] â†’ [LM Studio:1234]
    â†‘                                                       â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [LangChain Response] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¸ Como Usar

1. Acesse `http://localhost:3000`
2. Selecione os gÃªneros que gosta
3. **Tire uma foto da sua estante** (pelo navegador)
4. Clique em "Buscar matches na estante"
5. Aguarde o processamento (pode levar 10-30 segundos)

**Nota**: A foto Ã© enviada pelo navegador, *nÃ£o* diretamente no LM Studio.

## ğŸ› Troubleshooting

### "Failed to fetch"
- Verifique se o LM Studio estÃ¡ rodando
- Confirme que o servidor local estÃ¡ verde
- Reinicie o servidor Vite: `npm run dev`

###" messages field is required"
- Verifique se o modelo estÃ¡ carregado corretamente
- Confirme que o `mmproj` estÃ¡ ativo
- Tente recarregar o modelo no LM Studio

### Resposta lenta
- Normal! Modelos locais demoram mais que APIs em nuvem
- GPU acelera, mas CPU funciona (mais lento)
- Imagens grandes tomam mais tempo

## ğŸ“¦ DependÃªncias Instaladas

```json
{
  "langchain": "^0.x.x",
  "@langchain/core": "^0.x.x",
  "@langchain/community": "^0.x.x"
}
```

## ğŸ” Logs de Debug

O console do navegador mostrarÃ¡ logs do LangChain:
- `[LangChain] Initializing ChatOpenAI model...`
- `[LangChain] Creating vision message...`
- `[LangChain] Invoking model...`
- `[LangChain] Success!`

Monitore esses logs para identificar problemas.
